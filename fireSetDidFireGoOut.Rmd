---
title: "fireSet"
author: "Thomas Dahlgren & Josh Quist"
date: "4/28/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## dataframes 
```{r}
library(keras)
library(tfdatasets)
library(ggplot2)
library(tidyr)
library(tibble)
library(dplyr)



#Loading a csv file
firedf <- read.csv("firePutOut.csv")

#Removes lpg from our dataset since we don't want it to be part of the training, it uses different parameters
#Only use if using the same dataset that we used, if your dataset doesn't have lpg do not include this line
firedf <- firedf[1:15390,]

#Loading the dataset
firedf <- read.csv("firePutOut.csv")


# Various syntax for R dataframes
summary(firedf)


summary(firedf)
```

## Data preprocessing

```{r}

# Changing strings of fuel types to categories


catToNum1 <- function(c) {
  ifelse (c == "gasoline",0,c) 
}


catToNum2 <- function(c) {
  ifelse (c == "thinner",1,c)
}

catToNum3 <- function(c) {
  ifelse (c == "kerosene",2,c)
}

# Testing the functions

catToNum1("gasoline")
catToNum2("thinner")
catToNum3("kerosene")

# Note: changes it in place! 
# Also the function is applied to the entire column - many (but not all) functions in R may be used this way
firedf[2] <- lapply(firedf[2],catToNum1)
firedf[2] <- lapply(firedf[2],catToNum2)
firedf[2] <- lapply(firedf[2],catToNum3)
firedf[2] <- lapply(firedf[2],strtoi) # Apparently the category numbers in firedf[5] are stored as strings; convert to numbers

#Convert all the data to floats between 0 and 1
firedf[3] <- firedf[3]/max(firedf[3])
firedf[4] <- firedf[4]/max(firedf[4])
firedf[5] <- firedf[5]/max(firedf[5])
firedf[6] <- firedf[6]/max(firedf[6])

head(firedf)

```


```{r}
#Map the correlations of the data columns
#Can see everything except FUEL has a correlation to STATUS
library(corrplot)
firedf.cor = cor(firedf)
firedf.cor
corrplot(firedf.cor)
```

## Splitting data into training and testing

```{r}
sample_size <- 12000
set.seed(1234) # setting random seed to make results repeatable

picked <- sample(seq_len(nrow(firedf)),size = sample_size)
training <- firedf[picked,]
testing <- firedf[-picked,]

xTr <- as.matrix(training[c(1,3:6)])
yTr <- as.matrix(training[,7:7])
xTest <- as.matrix(testing[c(1,3:6)])
yTest <- as.matrix(testing[,7:7])


```


## Neural network Setup

```{r}

#64 nodes
first_model = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


first_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

first_history <- first_model %>% 
  fit(
    x = xTr, # input is the first 6 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 30,
    validation_data = list(xTest, yTest),
    verbose = 2
  )
```



## Testing model with dropout

```{r}
#64 nodes
dropout_model = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


dropout_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

dropout_history <- dropout_model %>% 
  fit(
    x = xTr, # input is the first 6 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 30,
    validation_data = list(xTest, yTest),
    verbose = 2
  )

```

#Using L2 regularization

```{r}
#64 nodes
l2_model = keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 1, activation = "sigmoid")


l2_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)


l2_history <- l2_model %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 30,
    validation_data = list(xTest, yTest),
    verbose = 2
  )
```



## Plotting loss for all 3 models

```{r}
compare_cx <- data.frame(
  first_train = first_history$metrics$loss,
  first_val = first_history$metrics$val_loss,
  dropout_train = dropout_history$metrics$loss,
  dropout_val = dropout_history$metrics$val_loss,
  regularization_train = l2_history$metrics$loss,
  regularization_val = l2_history$metrics$loss

) %>%
  rownames_to_column() %>%
  mutate(rowname = as.integer(rowname)) %>%
  gather(key = "type", value = "value", -rowname)
  
ggplot(compare_cx, aes(x = rowname, y = value, color = type)) +
  geom_line() +
  xlab("epoch") +
  ylab("loss")
```

# Evaluate the models
```{r}
first_model %>% evaluate(xTest, yTest)
dropout_model %>% evaluate(xTest, yTest)
l2_model %>% evaluate(xTest, yTest)

```
