---
title: "fireSet"
author: "Thomas Dahlgren"
date: "4/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## dataframes 
```{r}
library(keras)
library(tfdatasets)
library(ggplot2)
library(tidyr)
library(tibble)
library(dplyr)


#Loading a csv file
irisdf <- read.csv("firePutOut.csv")

# Various syntax for R dataframes
summary(irisdf)

head(irisdf)

# You can refer to columns by name or by number:
head(irisdf[1]) # head takes the first few elements

head(irisdf$DESIBEL)

head(irisdf["FUEL"])
```


```{r}
#You can refer to rows by numbers, note difference from columns:
irisdf[5,]

#Ranges of columns:
head(irisdf[1:3])

#Ranges of rows:
irisdf[1:3,]
```

## Data preprocessing

```{r}

# Changing strings to categories

# Nested ifelse doesn't work on a column, so we use 3 functions, applied one by one: 

catToNum1 <- function(c) {
  ifelse (c == "gasoline",0,c) 
}


catToNum2 <- function(c) {
  ifelse (c == "thinner",1,c)
}

catToNum3 <- function(c) {
  ifelse (c == "kerosene",2,c)
}
catToNum4 <- function(c) {
  ifelse (c == "lpg",3,c)
}

# Testing the functions

catToNum1("gasoline")
catToNum2("thinner")
catToNum3("kerosene")
catToNum4("lpg")

# Note: changes it in place! 
# Also the function is applied to the entire column - many (but not all) functions in R may be used this way
irisdf[2] <- lapply(irisdf[2],catToNum1)
irisdf[2] <- lapply(irisdf[2],catToNum2)
irisdf[2] <- lapply(irisdf[2],catToNum3)
irisdf[2] <- lapply(irisdf[2],catToNum4)
irisdf[2] <- lapply(irisdf[2],strtoi) # Apparently the category numbers in irisdf[5] are stored as strings; convert to numbers
irisdf[3] <- irisdf[3]/max(irisdf[3])
irisdf[4] <- irisdf[4]/max(irisdf[4])
irisdf[5] <- irisdf[5]/max(irisdf[5])
irisdf[6] <- irisdf[6]/max(irisdf[6])

head(irisdf)
```

## Splitting data into training and testing

```{r}
sample_size <- 14000
set.seed(1234) # setting random seed to make results repeatable

picked <- sample(seq_len(nrow(irisdf)),size = sample_size)
training <- irisdf[picked,]
testing <- irisdf[-picked,]

#write.csv(training,"training.csv", row.names = FALSE)
#write.csv(testing,"testing.csv", row.names = FALSE)

# Changing y into categorical data (performing one-hot encoding)

#yTr <- to_categorical(training$STATUS, num_classes = 2)
#yTest <- to_categorical(testing$STATUS, num_classes = 2)





```

## Neural network for the iris example
```{r}

#64 nodes
first_model = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


first_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

xTr <- as.matrix(training[,1:6])
yTr <- as.matrix(training[,7:7])
xTest <- as.matrix(testing[,1:6])
yTest <- as.matrix(testing[,7:7])

first_history <- first_model %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 50,
    validation_data = list(xTest, yTest),
    verbose = 2
  )
```


```{r}
#64 nodes
dropout_model = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


dropout_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

xTr <- as.matrix(training[,1:6])
yTr <- as.matrix(training[,7:7])
xTest <- as.matrix(testing[,1:6])
yTest <- as.matrix(testing[,7:7])

dropout_history <- dropout_model %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 50,
    validation_data = list(xTest, yTest),
    verbose = 2
  )

```
```{r}
#64 nodes
dropout_model = keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 1, activation = "sigmoid")


dropout_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

xTr <- as.matrix(training[,1:6])
yTr <- as.matrix(training[,7:7])
xTest <- as.matrix(testing[,1:6])
yTest <- as.matrix(testing[,7:7])

dropout_history <- dropout_model %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 50,
    validation_data = list(xTest, yTest),
    verbose = 2
  )
```



```{r}
compare_cx <- data.frame(
  first_train = first_history$metrics$loss,
  first_val = first_history$metrics$val_loss,
  dropout_train = dropout_history$metrics$loss,
  dropout_val = dropout_history$metrics$val_loss

) %>%
  rownames_to_column() %>%
  mutate(rowname = as.integer(rowname)) %>%
  gather(key = "type", value = "value", -rowname)
  
ggplot(compare_cx, aes(x = rowname, y = value, color = type)) +
  geom_line() +
  xlab("epoch") +
  ylab("loss")
```

# Evaluate the model
```{r}
first_model %>% evaluate(xTest, yTest)
dropout_model %>% evaluate(xTest, yTest)

#predictions <- predict(model, xTest)
#head(predictions, 2)

#testing[,7][1:2]

```
