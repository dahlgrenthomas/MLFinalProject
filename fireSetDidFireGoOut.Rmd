---
title: "fireSet"
author: "Thomas Dahlgren & Josh Quist"
date: "4/28/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## dataframes 
```{r}
library(keras)
library(tfdatasets)
library(ggplot2)
library(tidyr)
library(tibble)
library(dplyr)



#Loading a csv file
irisdf <- read.csv("firePutOut.csv")

#Removes lpg from our dataset since we don't want it to be part of the training, it uses different parameters
#Only use if using the same dataset that we used, if your dataset doesn't have lpg do not include this line
irisdf <- irisdf[1:15390,]

#Loading the dataset
firedf <- read.csv("firePutOut.csv")


# Various syntax for R dataframes
summary(irisdf)


summary(firedf)
```

## Data preprocessing

```{r}

# Changing strings of fuel types to categories


catToNum1 <- function(c) {
  ifelse (c == "gasoline",0,c) 
}


catToNum2 <- function(c) {
  ifelse (c == "thinner",1,c)
}

catToNum3 <- function(c) {
  ifelse (c == "kerosene",2,c)
}

# Testing the functions

catToNum1("gasoline")
catToNum2("thinner")
catToNum3("kerosene")

# Note: changes it in place! 
# Also the function is applied to the entire column - many (but not all) functions in R may be used this way
irisdf[2] <- lapply(irisdf[2],catToNum1)
irisdf[2] <- lapply(irisdf[2],catToNum2)
irisdf[2] <- lapply(irisdf[2],catToNum3)
irisdf[2] <- lapply(irisdf[2],strtoi) # Apparently the category numbers in irisdf[5] are stored as strings; convert to numbers

#Convert all the data to floats between 0 and 1
irisdf[3] <- irisdf[3]/max(irisdf[3])
irisdf[4] <- irisdf[4]/max(irisdf[4])
irisdf[5] <- irisdf[5]/max(irisdf[5])
irisdf[6] <- irisdf[6]/max(irisdf[6])

head(irisdf)

```
```{r}
#Map the correlations of the data columns
#Can see everything except FUEL has a correlation to STATUS
library(corrplot)
irisdf.cor = cor(irisdf)
irisdf.cor
corrplot(irisdf.cor)
```

## Splitting data into training and testing

```{r}
sample_size <- 12000
set.seed(1234) # setting random seed to make results repeatable

picked <- sample(seq_len(nrow(firedf)),size = sample_size)
training <- firedf[picked,]
testing <- firedf[-picked,]

xTr <- as.matrix(training[c(1,3:6)])
yTr <- as.matrix(training[,7:7])
xTest <- as.matrix(testing[c(1,3:6)])
yTest <- as.matrix(testing[,7:7])


```

## Basic neural network
#write.csv(training,"training.csv", row.names = FALSE)
#write.csv(testing,"testing.csv", row.names = FALSE)


# Changing y into categorical data (performing one-hot encoding)

#yTr <- to_categorical(training$STATUS, num_classes = 2)
#yTest <- to_categorical(testing$STATUS, num_classes = 2)
```

## Neural network Setup
```{r}

#64 nodes
first_model = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


first_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

first_history <- first_model %>% 
  fit(
    x = xTr, # input is the first 6 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 30,
    validation_data = list(xTest, yTest),
    verbose = 2
  )
```

#Using dropout

## Testing model with dropout

```{r}
#64 nodes
dropout_model = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


dropout_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

#xTr <- as.matrix(training[,1:6])
#yTr <- as.matrix(training[,7:7])
#xTest <- as.matrix(testing[,1:6])
#yTest <- as.matrix(testing[,7:7])

dropout_history <- dropout_model %>% 
  fit(
    x = xTr, # input is the first 6 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 30,
    validation_data = list(xTest, yTest),
    verbose = 2
  )

```

#Using L2 regularization


## Model testing regularization
```{r}
#64 nodes
l2_model = keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 1, activation = "sigmoid")


l2_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)


l2_history <- l2_model %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 30,
    validation_data = list(xTest, yTest),
    verbose = 2
  )
```



#Plotting the loss the three neural networks

## Plotting loss for all 3 models

```{r}
compare_cx <- data.frame(
  first_train = first_history$metrics$loss,
  first_val = first_history$metrics$val_loss,
  dropout_train = dropout_history$metrics$loss,
  dropout_val = dropout_history$metrics$val_loss,
  regularization_train = l2_history$metrics$loss,
  regularization_val = l2_history$metrics$loss

) %>%
  rownames_to_column() %>%
  mutate(rowname = as.integer(rowname)) %>%
  gather(key = "type", value = "value", -rowname)
  
ggplot(compare_cx, aes(x = rowname, y = value, color = type)) +
  geom_line() +
  xlab("epoch") +
  ylab("loss")
```

# Evaluate the models
```{r}
first_model %>% evaluate(xTest, yTest)
dropout_model %>% evaluate(xTest, yTest)
l2_model %>% evaluate(xTest, yTest)


```
