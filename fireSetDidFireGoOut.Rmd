---
title: "fireSet"
author: "Thomas Dahlgren"
date: "4/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## dataframes 
```{r}
library(keras)
library(tfdatasets)
library(ggplot2)
library(tidyr)
library(tibble)
library(dplyr)


#Loading a csv file
irisdf <- read.csv("firePutOut.csv")

#Removes lpg from our dataset since we don't want it to be part of the training, it uses different parameters
#Only use if using the same dataset that we used, if your dataset doesn't have lpg do not include this line
irisdf <- irisdf[1:15390,]

# Various syntax for R dataframes
summary(irisdf)

head(irisdf)

# You can refer to columns by name or by number:
head(irisdf[1]) # head takes the first few elements

head(irisdf$DESIBEL)

head(irisdf["FUEL"])
```


```{r}
#You can refer to rows by numbers, note difference from columns:
irisdf[5,]

#Ranges of columns:
head(irisdf[1:3])

#Ranges of rows:
irisdf[1:3,]
```

## Data preprocessing

```{r}

# Changing strings to categories

# Nested ifelse doesn't work on a column, so we use 3 functions, applied one by one: 

catToNum1 <- function(c) {
  ifelse (c == "gasoline",0,c) 
}


catToNum2 <- function(c) {
  ifelse (c == "thinner",1,c)
}

catToNum3 <- function(c) {
  ifelse (c == "kerosene",2,c)
}

# Testing the functions

catToNum1("gasoline")
catToNum2("thinner")
catToNum3("kerosene")

# Note: changes it in place! 
# Also the function is applied to the entire column - many (but not all) functions in R may be used this way
irisdf[2] <- lapply(irisdf[2],catToNum1)
irisdf[2] <- lapply(irisdf[2],catToNum2)
irisdf[2] <- lapply(irisdf[2],catToNum3)
irisdf[2] <- lapply(irisdf[2],strtoi) # Apparently the category numbers in irisdf[5] are stored as strings; convert to numbers

#Convert all the data to floats between 0 and 1
irisdf[3] <- irisdf[3]/max(irisdf[3])
irisdf[4] <- irisdf[4]/max(irisdf[4])
irisdf[5] <- irisdf[5]/max(irisdf[5])
irisdf[6] <- irisdf[6]/max(irisdf[6])

head(irisdf)
```
```{r}
#Map the correlations of the data columns
#Can see everything except FUEL has a correlation to STATUS
library(corrplot)
irisdf.cor = cor(irisdf)
irisdf.cor
corrplot(irisdf.cor)
```

## Splitting data into training and testing

```{r}
sample_size <- 12000
set.seed(1234) # setting random seed to make results repeatable

picked <- sample(seq_len(nrow(irisdf)),size = sample_size)
training <- irisdf[picked,]
testing <- irisdf[-picked,]

xTr <- as.matrix(training[c(1,3:6)])
yTr <- as.matrix(training[,7:7])
xTest <- as.matrix(testing[c(1,3:6)])
yTest <- as.matrix(testing[,7:7])

```

## Basic neural network
```{r}

#64 nodes
first_model = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


first_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

first_history <- first_model %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 30,
    validation_data = list(xTest, yTest),
    verbose = 2
  )
```

#Using dropout
```{r}
#64 nodes
dropout_model = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


dropout_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

#xTr <- as.matrix(training[,1:6])
#yTr <- as.matrix(training[,7:7])
#xTest <- as.matrix(testing[,1:6])
#yTest <- as.matrix(testing[,7:7])

dropout_history <- dropout_model %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 30,
    validation_data = list(xTest, yTest),
    verbose = 2
  )

```
#Using L2 regularization
```{r}
#64 nodes
l2_model = keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 1, activation = "sigmoid")


l2_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)


l2_history <- l2_model %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 30,
    validation_data = list(xTest, yTest),
    verbose = 2
  )
```


#Plotting the loss the three neural networks
```{r}
compare_cx <- data.frame(
  first_train = first_history$metrics$loss,
  first_val = first_history$metrics$val_loss,
  dropout_train = dropout_history$metrics$loss,
  dropout_val = dropout_history$metrics$val_loss

) %>%
  rownames_to_column() %>%
  mutate(rowname = as.integer(rowname)) %>%
  gather(key = "type", value = "value", -rowname)
  
ggplot(compare_cx, aes(x = rowname, y = value, color = type)) +
  geom_line() +
  xlab("epoch") +
  ylab("loss")
```

# Evaluate the models
```{r}
first_model %>% evaluate(xTest, yTest)
dropout_model %>% evaluate(xTest, yTest)
l2_model %>% evaluate(xTest, yTest)


```
