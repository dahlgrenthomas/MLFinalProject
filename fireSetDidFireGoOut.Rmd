---
title: "fireSet"
author: "Thomas Dahlgren & Josh Quist"
date: "4/28/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## dataframes 
```{r}
library(keras)
library(tfdatasets)
library(ggplot2)
library(tidyr)
library(tibble)
library(dplyr)


#Loading the dataset
firedf <- read.csv("firePutOut.csv")

# Data summary
head(firedf)

summary(firedf)
```

## Data preprocessing

```{r}

# Changing strings of fuel types to categories


catToNum1 <- function(c) {
  ifelse (c == "gasoline",0,c) 
}


catToNum2 <- function(c) {
  ifelse (c == "thinner",1,c)
}

catToNum3 <- function(c) {
  ifelse (c == "kerosene",2,c)
}
catToNum4 <- function(c) {
  ifelse (c == "lpg",3,c)
}

# Testing the functions

catToNum1("gasoline")
catToNum2("thinner")
catToNum3("kerosene")
catToNum4("lpg")

# Apply each function to the fuel column; setting each fuel to a corresponding number
firedf[2] <- lapply(firedf[2],catToNum1)
firedf[2] <- lapply(firedf[2],catToNum2)
firedf[2] <- lapply(firedf[2],catToNum3)
firedf[2] <- lapply(firedf[2],catToNum4)



# Processing data using 
firedf[3] <- firedf[3]/max(firedf[3])
firedf[4] <- firedf[4]/max(firedf[4])
firedf[5] <- firedf[5]/max(firedf[5])
firedf[6] <- firedf[6]/max(firedf[6])

head(firedf)
```

## Splitting data into training and testing

```{r}
sample_size <- 14000
set.seed(1234) # setting random seed to make results repeatable

picked <- sample(seq_len(nrow(firedf)),size = sample_size)
training <- firedf[picked,]
testing <- firedf[-picked,]


# Changing y into categorical data (performing one-hot encoding)

#yTr <- to_categorical(training$STATUS, num_classes = 2)
#yTest <- to_categorical(testing$STATUS, num_classes = 2)
```

## Neural network Setup
```{r}

#64 nodes
first_model = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


first_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

xTr <- as.matrix(training[,1:6])
yTr <- as.matrix(training[,7:7])
xTest <- as.matrix(testing[,1:6])
yTest <- as.matrix(testing[,7:7])

first_history <- first_model %>% 
  fit(
    x = xTr, # input is the first 6 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 20,
    validation_data = list(xTest, yTest),
    verbose = 2
  )
```


## Testing model with dropout
```{r}
#64 nodes
dropout_model = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(0.2) %>% 
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")


dropout_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

xTr <- as.matrix(training[,1:6])
yTr <- as.matrix(training[,7:7])
xTest <- as.matrix(testing[,1:6])
yTest <- as.matrix(testing[,7:7])

dropout_history <- dropout_model %>% 
  fit(
    x = xTr, # input is the first 6 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 20,
    validation_data = list(xTest, yTest),
    verbose = 2
  )

```

## Model testing regularization
```{r}
#64 nodes
l2_model = keras_model_sequential() %>%
    layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 64, activation = "relu",
              kernel_regularizer = regularizer_l2(l = 0.001)) %>%
  layer_dense(units = 1, activation = "sigmoid")


l2_model %>% compile(
  loss = "binary_crossentropy",
  metrics = list("binary_accuracy"),
  optimizer = "adam"
  
)

xTr <- as.matrix(training[,1:6])
yTr <- as.matrix(training[,7:7])
xTest <- as.matrix(testing[,1:6])
yTest <- as.matrix(testing[,7:7])

l2_history <- l2_model %>% 
  fit(
    x = xTr, # input is the first 4 columns of the dataframe
    y = yTr, # label is the last column
    epochs = 20,
    validation_data = list(xTest, yTest),
    verbose = 2
  )
```


## Plotting loss for all 3 models
```{r}
compare_cx <- data.frame(
  first_train = first_history$metrics$loss,
  first_val = first_history$metrics$val_loss,
  dropout_train = dropout_history$metrics$loss,
  dropout_val = dropout_history$metrics$val_loss,
  regularization_train = l2_history$metrics$loss,
  regularization_val = l2_history$metrics$loss

) %>%
  rownames_to_column() %>%
  mutate(rowname = as.integer(rowname)) %>%
  gather(key = "type", value = "value", -rowname)
  
ggplot(compare_cx, aes(x = rowname, y = value, color = type)) +
  geom_line() +
  xlab("epoch") +
  ylab("loss")
```

# Evaluate the models
```{r}
first_model %>% evaluate(xTest, yTest)
dropout_model %>% evaluate(xTest, yTest)
l2_model %>% evaluate(xTest, yTest)

#predictions <- predict(model, xTest)
#head(predictions, 2)

#testing[,7][1:2]

```
